# LLM BENCHMARKING WITH LLAMA2: EVALUATING CODE DEVELOPMENT PERFORMANCE ACROSS MULTIPLE PROGRAMMING LANGUAGES
 <a href="https://arxiv.org/abs/2503.19217"><img src="https://img.shields.io/badge/arXiv-2503.19217-b31b1b.svg?style=for-the-badge"> [![DOI](https://zenodo.org/badge/852986173.svg)](https://doi.org/10.5281/zenodo.15832248)

## Content

* Generated coded in Section 4 are available [here](https://github.com/diehlpkpapers/ai-journal-paper)
* Generated code document ion in Section 6 is available [here](https://github.com/diehlpkpapers/ai-journal-paper/tree/main/documentation)
* Generated unit tests in Section 8 are available [here](https://github.com/diehlpkpapers/ai-journal-paper/tree/main/testing)
* Translated code in Section 10 are available [here](https://github.com/diehlpkpapers/ai-journal-paper/tree/main/translation)

## References

* [Preprint](https://arxiv.org/abs/2503.19217)
